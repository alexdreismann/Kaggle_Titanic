{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Titanic' challenge of Kaggle\n",
    "\n",
    "The 'Titanic' data challenge from Kaggle, done on my own. I'll attempt to use multiple linear regression as described in 'Data science from scratch'. \n",
    "\n",
    "Description of the challenge from Kaggle: \"One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\"\n",
    "\n",
    "NB: Another notebook on linear regression with Python: http://www.dataschool.io/linear-regression-in-python/\n",
    "\n",
    "\n",
    "**To do**\n",
    "* See how the model behaves if different parameters are taken into account (only one column, etc.). (The model seems to be almost as good if only the 'Sex' column is taken into account, which is a bit worriesome...).\n",
    "* See what I can learn about the significance of the different parameters if I look at standard errors of the fits.\n",
    "* See how good predictions are compared to other predictions on the leaderboard.\n",
    "* Think: Why would linear regression actually make sense?\n",
    "* Can one extract how much each parameter contributes to the survival? Would need some sort of normalisation, since some values (e.g. 'Age') are much larger than others (e.g. 'Sex')\n",
    "\n",
    "\n",
    "### Import stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data set and have a first look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male   22      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
      "2                             Heikkinen, Miss. Laina  female   26      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
      "4                           Allen, Mr. William Henry    male   35      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "Testing data set:\n",
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n",
      "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
      "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
      "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
      "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
      "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
      "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
      "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
      "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
      "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "titanic_train_original = pd.read_csv('train.csv')\n",
    "titanic_test_original = pd.read_csv('test.csv')\n",
    "print ('Training data set:')\n",
    "print (titanic_train_original.head())\n",
    "print (titanic_train_original.describe())\n",
    "print ('Testing data set:')\n",
    "print (titanic_test_original.head())\n",
    "print (titanic_test_original.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more column in 'titanic_train' than in 'titanic_test', namely the 'survived' column. This is the case because this is what the model needs to predict and what one needs to submit to kaggle. \n",
    "\n",
    "Some of the columns are not numeric data, namely 'Name', 'Sex', 'Ticket', 'Cabin' and 'Embarked'. Some of these columns are probabily worth ignoring, since it will be hard to obtain meaningful information from them. The once which might be useful - for the moment we will use 'Sex' and 'Embarked' - need to be transformed to numerical data to implement the fit. To replace all values with numbers, first determine which values exist in the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n",
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "print (titanic_train_original['Sex'].unique())\n",
    "print (titanic_train_original['Embarked'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following replacement rules:\n",
    "\n",
    "In 'Sex', replace 'male' => 0 and 'female' => 1. \n",
    "In 'Embarked', replace 'S' => 0, 'C' => 1 and 'Q' => 2.\n",
    "\n",
    "To leave the original data frames intact, I will make copies of the two data sets for further manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Sex         Age  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  714.000000   \n",
      "mean    446.000000    0.383838    2.308642    0.352413   29.699118   \n",
      "std     257.353842    0.486592    0.836071    0.477990   14.526497   \n",
      "min       1.000000    0.000000    1.000000    0.000000    0.420000   \n",
      "25%     223.500000    0.000000    2.000000    0.000000   20.125000   \n",
      "50%     446.000000    0.000000    3.000000    0.000000   28.000000   \n",
      "75%     668.500000    1.000000    3.000000    1.000000   38.000000   \n",
      "max     891.000000    1.000000    3.000000    1.000000   80.000000   \n",
      "\n",
      "            SibSp       Parch        Fare    Embarked  \n",
      "count  891.000000  891.000000  891.000000  889.000000  \n",
      "mean     0.523008    0.381594   32.204208    0.362205  \n",
      "std      1.102743    0.806057   49.693429    0.636157  \n",
      "min      0.000000    0.000000    0.000000    0.000000  \n",
      "25%      0.000000    0.000000    7.910400    0.000000  \n",
      "50%      0.000000    0.000000   14.454200    0.000000  \n",
      "75%      1.000000    0.000000   31.000000    1.000000  \n",
      "max      8.000000    6.000000  512.329200    2.000000  \n",
      "       PassengerId      Pclass         Sex         Age       SibSp  \\\n",
      "count   418.000000  418.000000  418.000000  332.000000  418.000000   \n",
      "mean   1100.500000    2.265550    0.363636   30.272590    0.447368   \n",
      "std     120.810458    0.841838    0.481622   14.181209    0.896760   \n",
      "min     892.000000    1.000000    0.000000    0.170000    0.000000   \n",
      "25%     996.250000    1.000000    0.000000   21.000000    0.000000   \n",
      "50%    1100.500000    3.000000    0.000000   27.000000    0.000000   \n",
      "75%    1204.750000    3.000000    1.000000   39.000000    1.000000   \n",
      "max    1309.000000    3.000000    1.000000   76.000000    8.000000   \n",
      "\n",
      "            Parch        Fare    Embarked  \n",
      "count  418.000000  417.000000  418.000000  \n",
      "mean     0.392344   35.627188    0.464115  \n",
      "std      0.981429   55.907576    0.685516  \n",
      "min      0.000000    0.000000    0.000000  \n",
      "25%      0.000000    7.895800    0.000000  \n",
      "50%      0.000000   14.454200    0.000000  \n",
      "75%      0.000000   31.500000    1.000000  \n",
      "max      9.000000  512.329200    2.000000  \n"
     ]
    }
   ],
   "source": [
    "# Make copies of original data frames for further manipulation\n",
    "titanic_train = titanic_train_original.copy()\n",
    "titanic_test = titanic_test_original.copy()\n",
    "\n",
    "titanic_train['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "titanic_test['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "titanic_train['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n",
    "titanic_test['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n",
    "\n",
    "print (titanic_train.describe())\n",
    "print (titanic_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are values missing (i.e. NA, null or NaN) in several columns (wherever 'count' is smaller than 891 in the training data and smaller than 418 in the testing data). To further treat the data, I will fill in some values for those (since we don't want to throw away the corresponding rows). An accepted way of doing so is to replace the missing values with the median of the corresponding column in the case of numerical data. For the non-numerical columns, I will use the most frequent value of the column.\n",
    "\n",
    "Note that for the test data set, I will use the median of the training data to replace fitting values. Not sure why this is done, but apparently it is the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    644\n",
      "1    168\n",
      "2     77\n",
      "Name: Embarked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# replace missing values in numerical columns with median of column\n",
    "titanic_train['Age'].fillna(titanic_train['Age'].median(),inplace=True)\n",
    "titanic_test['Age'].fillna(titanic_train['Age'].median(),inplace=True)\n",
    "titanic_test['Fare'].fillna(titanic_train['Fare'].median(),inplace=True)\n",
    "\n",
    "# replace missing values of train['Embarked']\n",
    "print(titanic_train['Embarked'].value_counts())\n",
    "titanic_train['Embarked'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using numpy.linalg.lstsq\n",
    "\n",
    "From the Numpy documentation: \"numpy.linalg.lstsq(a, b, rcond=-1): Return the least-squares solution to a linear matrix equation.\n",
    "\n",
    "Solves the equation a x = b by computing a vector x that minimizes the Euclidean 2-norm || b - a x ||^2. The equation may be under-, well-, or over- determined (i.e., the number of linearly independent rows of a can be less than, equal to, or greater than its number of linearly independent columns). If a is square and of full rank, then x (but for round-off error) is the “exact” solution of the equation.\"\n",
    "\n",
    "To use this, I probably need to create numpy arrays for a (all the parameters) and b (survived column). The resulting x would give the parameters of the prediction model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (titanic_train.columns[:])\n",
    "a = titanic_train.as_matrix(columns=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'])\n",
    "\n",
    "# add a first column of '1' for the constant of the linear fit\n",
    "a = np.concatenate((np.ones((891,1)),a),axis=1)\n",
    "\n",
    "b = titanic_train.as_matrix(columns=['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to use the least square fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.86246139e-01]\n",
      " [ -1.75625202e-01]\n",
      " [  5.04721091e-01]\n",
      " [ -5.85421870e-03]\n",
      " [ -4.21320820e-02]\n",
      " [ -1.51876675e-02]\n",
      " [  3.30107827e-04]\n",
      " [  3.94775733e-02]]\n"
     ]
    }
   ],
   "source": [
    "beta_np, _, _ ,_ = np.linalg.lstsq(a,b)\n",
    "print (beta_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like this, it is a bit hard to tell if this is doing good or not, because I cannot test my prediction algorithm on anything (since I have used all my training data for the fit and do not know the results for the testing data). A better way of doing this is **cross validation**, which means that I separate my training data in multiple data sets, parts of which I can then use for training and parts for testing. Additionally, this allows checking how stable the different parameters are (e.g. if the parameter for 'Age' has a large standard deviation, it means that is probably not playing a role..).\n",
    "\n",
    "However, since all of this is possibly a bit annoying to do in Numpy and by hand, I will next have a look at the build-in module of sci-kit, which apparently provides this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using sklearn\n",
    "\n",
    "Documentation for the sklearn linear regression module: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Documentation for the sklearn cross_validation module: http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "**Import relevant modules:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'LinearRegression' is a class, so I first need to initiate an instance of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialise LinearRegression class\n",
    "LR = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the documentation on cross-validation: \"In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "* A model is trained using k-1 of the folds as training data;\n",
    "* The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop.\"\n",
    "\n",
    "And from the documentation of kFold: \"KFold divides all the samples in k groups of samples, called folds, of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test.\" \n",
    "See: http://scikit-learn.org/stable/modules/cross_validation.html for how exactly it works. If I understand correclty, it produces indices which one can use to reference the training and testing set when doing the cross validation. \n",
    "\n",
    "The inputs of KFold are the number of rows and the number of folds that should be created. Additionally, specifying 'random_state = const.' fixes the folds so they are the same with each iteration of the code. Otherwise, folds are selected randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate cross validation folds for the titanic_train data set.\n",
    "# KF returns indices to use in each iteration of the cross validation\n",
    "KF = KFold(len(titanic_train),n_folds=5,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define the columns of the data set (i.e. the parameters) which I want to use for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select columns of 'titanic_train' which should be used for fitting. \n",
    "predictors = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
    "#predictors = ['Pclass','Sex','Age','Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the algorithm. The way it is done here is for educational purposes.\n",
    "* First create a container for the predictions made with each of the 3 kFolds.\n",
    "* Loop through the 3 kFolds and use the indices to select the respective training and testing data.\n",
    "* Fit using the training data.\n",
    "* Apply fit to the testing data and obtain predictions. \n",
    "* Combine all predictions and compare them to the predictions given in 'titanic_train'.\n",
    "\n",
    "NB: I realise that the 'LinearRegression' does actually not have a function to print the coefficients obtained from the fit. These are instead 'attributes of the estimator' (I believe that is the instance of the class). However, these are also easily accessible, e.g. by printing 'LR.coef_' (type LR. and press 'tab' to get a list of attributes, or LR and 'tab' to get a list of functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.94770155e-01   4.83242593e-01  -5.87181818e-03  -3.01200776e-02\n",
      "   -2.51506883e-02   3.99245080e-04   3.78545461e-02]\n",
      " [ -1.90435635e-01   5.04925548e-01  -5.86745877e-03  -5.44579378e-02\n",
      "   -1.53140798e-02   2.95697260e-04   3.23947446e-02]\n",
      " [ -1.60392743e-01   5.05703969e-01  -6.10566228e-03  -4.25034053e-02\n",
      "   -2.19101463e-02   6.09333995e-04   4.10605535e-02]\n",
      " [ -1.54380130e-01   5.26348045e-01  -6.44693377e-03  -5.31316455e-02\n",
      "    1.79641915e-02   2.31350887e-04   4.58609934e-02]\n",
      " [ -1.78497200e-01   5.06138598e-01  -5.12500810e-03  -3.28888923e-02\n",
      "   -2.50505591e-02   5.03408113e-05   4.00999964e-02]]\n",
      "[0.40437021248719235, 0.39759260778316974, 0.40037302420560139, 0.41176701153299311, 0.38222953939333115]\n"
     ]
    }
   ],
   "source": [
    "# container for predictions\n",
    "predictions = []\n",
    "# container for fitting parameters\n",
    "coefficients = []\n",
    "# container for errors \n",
    "R2 = []\n",
    "\n",
    "# loop through kFolds\n",
    "for train, test in KF:\n",
    "    # select rows for training and columns defined in the 'predictors' above\n",
    "    train_predictors = titanic_train[predictors].iloc[train,:]\n",
    "    # select rows from 'survived' column as target for training\n",
    "    train_target = titanic_train['Survived'].iloc[train]\n",
    "    # obtain linear fit\n",
    "    LR.fit(train_predictors,train_target)\n",
    "    # determine R2 of the fit\n",
    "    train_R2 = LR.score(train_predictors,train_target)\n",
    "    R2.append(train_R2)\n",
    "    # extract coefficients of fit\n",
    "    coefficients.append(LR.coef_)\n",
    "    # use fit to make predictions for the test fold\n",
    "    test_predictions = LR.predict(titanic_train[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "#  concatenate 'predictions' and 'coefficients'\n",
    "predictions = np.concatenate(predictions,axis=0)\n",
    "coefficients = np.array(coefficients)\n",
    "print (coefficients)\n",
    "print (R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'predictions' initially contained three arrays of predictions for the three kFolds of 'titanic_train'. The way it is set up, these are already in order, so concatenating them gives one column which I can directly compare to 'titanic_train['Survived']. \n",
    "\n",
    "The coefficients of the linear fit are relatively constant for the different kFolds, which I assume is a good sign (if they were changing a lot, it would mean they are more or less random and do not reflect any significant relation).\n",
    "\n",
    "In the challenge the metric of success is defined as the fraction of correct predictions, so I can have a look at the training data and see how good I was doing here. Before this can be done, the predictions (which are floating point at the moment) need to be mapoped on the binary values '0' and '1'. This can be done by simply setting each value 0 if smaller than 0.5 and 1 if larger-equal 0.5.\n",
    "\n",
    "NB: Following 'Data Science from Scratch', the parameter 'accuracy' below is actually not a very good parameter... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "# map predictions on binary values 0 and 1\n",
    "predictions[predictions >= 0.5] = 1\n",
    "predictions[predictions < 0.5] = 0\n",
    "\n",
    "# sum the number of correct predictions and divide by total\n",
    "accuracy = np.sum([predictions == titanic_train['Survived']])/len(predictions)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a submission file for Kaggle\n",
    "\n",
    "The submission file should contain two columns only: Passanger IDs of the test set and the corresponding predictions. \n",
    "\n",
    "We have already cleaned up the test data set, so the only thing to do is running the linear regression on it (using the parameters from the training data) and make predictions. Since we satisfied ourselfs that the model works using the kFolds before, we'll now use the full training data set for fitting and then apply the obtained relation on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train algorithm on full training data set\n",
    "LR.fit(titanic_train[predictors],titanic_train['Survived'])\n",
    "# apply linear fit to test data set and make predictions\n",
    "predictions = LR.predict(titanic_test[predictors])\n",
    "# map predictions on 0 and 1\n",
    "predictions[predictions >= 0.5] = 1\n",
    "predictions[predictions < 0.5] = 0\n",
    "\n",
    "# generate a submission file, containing only passanger IDs and predictions\n",
    "predictions = pd.DataFrame(predictions)\n",
    "submission = pd.concat([titanic_test['PassengerId'],predictions],axis=1)\n",
    "submission.columns = ['PassengerId','Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('Titanic_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
